{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 2019 Liquor Sales in Iowa\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "We are going to look at spirits purchase information of Iowa Class “E” liquor licensees in 2019 that is pulically available.  We're going to create several dimension tables, as well as a sales summary table that we can feed into a visualization tool like Tableau for executives to look at sales trends.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading https://files.pythonhosted.org/packages/72/5c/ec84c7ec49fde2c3b0d885ecae4504fa40fc77fef7684e9f2939c50f9b94/s3fs-0.4.0-py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/1f/7028dacd3c28f34ce48130aae73a88fa5cc27b6b0e494fcf2739f7954d9d/fsspec-0.6.2-py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 6.1MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting boto3>=1.9.91 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/57/e9675a5a8d0ee586594ff19cb9a601334fbf24fa2fb29052d2a900ee5d23/boto3-1.11.9-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore>=1.12.91 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/4c/b0b0d3b6f84a05f9135051b56d3eb8708012a289c4b82ee21c8c766f47b5/botocore-1.14.9-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 4.1MB/s eta 0:00:01   9% |███                             | 552kB 23.4MB/s eta 0:00:01    92% |█████████████████████████████▌  | 5.4MB 35.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0 (from boto3>=1.9.91->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/48/a8252b6b3cd31774eab312b19d58a6ac55f296240c206617dcd38cd93bf8/s3transfer-0.3.2-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 16.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.91->s3fs) (0.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.6.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.22)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.11.0)\n",
      "\u001b[31mawscli 1.16.17 has requirement botocore==1.12.7, but you'll have botocore 1.14.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.16.17 has requirement s3transfer<0.2.0,>=0.1.12, but you'll have s3transfer 0.3.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fsspec, botocore, s3transfer, boto3, s3fs\n",
      "  Found existing installation: botocore 1.12.7\n",
      "    Uninstalling botocore-1.12.7:\n",
      "      Successfully uninstalled botocore-1.12.7\n",
      "  Found existing installation: s3transfer 0.1.13\n",
      "    Uninstalling s3transfer-0.1.13:\n",
      "      Successfully uninstalled s3transfer-0.1.13\n",
      "  Found existing installation: boto3 1.9.7\n",
      "    Uninstalling boto3-1.9.7:\n",
      "      Successfully uninstalled boto3-1.9.7\n",
      "Successfully installed boto3-1.11.9 botocore-1.14.9 fsspec-0.6.2 s3fs-0.4.0 s3transfer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import configparser\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "\n",
    "import s3fs\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']='XXX'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### 2019 liquor sales data.  This may take a few seconds.\n",
    "### https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy\n",
    "\n",
    "\n",
    "### NOTE! Since I couldn't submit with the raw csv, I zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/home/workspace/data/Iowa_Liquor_Sales.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------+--------------------+--------------------+------------+--------+--------------------+-------------+-------+--------+-----------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "|Invoice/Item Number|      Date|Store Number|          Store Name|             Address|        City|Zip Code|      Store Location|County Number| County|Category|    Category Name|Vendor Number|         Vendor Name|Item Number|    Item Description|Pack|Bottle Volume (ml)|State Bottle Cost|State Bottle Retail|Bottles Sold|Sale (Dollars)|Volume Sold (Liters)|Volume Sold (Gallons)|\n",
      "+-------------------+----------+------------+--------------------+--------------------+------------+--------+--------------------+-------------+-------+--------+-----------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "|    INV-16686100006|01/02/2019|        5637|   Catfish Charlie's|   1630 East 16th St|     Dubuque|   52001|                null|           31|DUBUQUE| 1012100|Canadian Whiskies|          260|     DIAGEO AMERICAS|      11297|         Crown Royal|  12|              1000|            18.89|              28.34|           1|         28.34|                1.00|                 0.26|\n",
      "|    INV-16676700025|01/02/2019|        2190|Central City Liqu...|        1460 2ND AVE|  Des Moines|   50314|POINT (-93.619787...|           77|   POLK| 1081200|   Cream Liqueurs|          305|             Mhw LTD|      73052|            Rumchata|  12|               375|             7.00|              10.50|           2|         21.00|                0.75|                 0.19|\n",
      "|    INV-16678600039|01/02/2019|        4641|Kum & Go #573 / S...|     5830 SE 14th St|  Des Moines|   50315|                null|           77|   POLK| 1031100|  American Vodkas|          301|FIFTH GENERATION INC|      38176|Titos Handmade Vodka|  12|               750|             9.64|              14.46|           6|         86.76|                4.50|                 1.18|\n",
      "|    INV-16679900011|01/02/2019|        5142|Smokin' Joe's #13...|2315 Mt Vernon Rd SE|Cedar Rapids|   52303|                null|           57|   Linn| 1012100|Canadian Whiskies|          260|     DIAGEO AMERICAS|      11299|         Crown Royal|  44|               200|             5.00|               7.50|           2|         15.00|                0.40|                 0.10|\n",
      "|    INV-16678600015|01/02/2019|        4641|Kum & Go #573 / S...|     5830 SE 14th St|  Des Moines|   50315|                null|           77|   POLK| 1032100|  Imported Vodkas|          370|   PERNOD RICARD USA|      34001|Absolut Swedish V...|  10|               600|             7.92|              11.88|           3|         35.64|                1.80|                 0.47|\n",
      "+-------------------+----------+------------+--------------------+--------------------+------------+--------+--------------------+-------------+-------+--------+-----------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sales\n",
    "    limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2380346, 24)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Median income and population data by zip code.  From here:\n",
    "### https://www.psc.isr.umich.edu/dis/census/Features/tract2zip/\n",
    "\n",
    "zip_df = spark.read.csv('/home/workspace/data/zip_median.csv', header=True)\n",
    "zip_df = (\n",
    "         zip_df\n",
    "         .withColumn('Pop', regexp_replace('Pop', ',', ''))\n",
    "         .withColumn('Median', regexp_replace('Median', ',', ''))\n",
    "     )\n",
    "zip_df.createOrReplaceTempView(\"zip_median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-----+\n",
      "| Zip|Median|  Mean|  Pop|\n",
      "+----+------+------+-----+\n",
      "|1001| 56663|66,688|16445|\n",
      "|1002| 49853|75,063|28069|\n",
      "|1003| 28462|35,121| 8491|\n",
      "|1005| 75423|82,442| 4798|\n",
      "|1007| 79076|85,802|12962|\n",
      "+----+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM zip_median\n",
    "    limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|              Zip|           Median|              Mean|               Pop|\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|            32634|            32634|             32634|             32634|\n",
      "|   mean|49875.28075013789|50938.20524606239|            536.55| 9192.768186553902|\n",
      "| stddev|27382.47649773278|20356.27230539215|254.20080645033366|13416.240474315491|\n",
      "|    min|            10001|           100000|                 .|                 1|\n",
      "|    max|            99929|            99994|            99,986|              9997|\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zip_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "We're going to look at both the sales dataframe and the zip code data frame to see if there are any null values that we need to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|count(1)|count(DISTINCT Zip)|\n",
      "+--------+-------------------+\n",
      "|   32634|              32634|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check the zip_median table to see if there are duplicate zip codes\n",
    "### to make sure we don't have problems during the join. \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT count(*),\n",
    "    count(distinct Zip)\n",
    "    FROM zip_median\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The zip code dataframe doesn't seem to have any columns that are null and all zips are unique, which is great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    4764|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### check to see if Zip Code value is null or blank\n",
    "spark.sql(\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM sales\n",
    "    where `Zip Code`='' or `Zip Code` is null\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Seems like all of sales table has a Zip Code value which is great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|          Store Name|count(1)|\n",
      "+--------------------+--------+\n",
      "|       A to Z Liquor|      89|\n",
      "|   Adventureland Inn|      14|\n",
      "|B and B EAST / Wa...|      45|\n",
      "|    Burlington Shell|      39|\n",
      "|CVS Pharmacy #101...|      13|\n",
      "|Casey's General S...|      70|\n",
      "|Casey's General S...|      31|\n",
      "|Casey's General S...|      69|\n",
      "|Casey's General S...|      36|\n",
      "|Casey's General S...|      23|\n",
      "|Chuck's Sportsman...|      96|\n",
      "|Clear Lake Payles...|      63|\n",
      "|Fareway Stores #0...|      54|\n",
      "|Fareway Stores #4...|     110|\n",
      "|Fareway Stores #7...|     178|\n",
      "|     Flashmart  #105|      11|\n",
      "|Flashmart #103/An...|      23|\n",
      "|       Freeman Foods|      73|\n",
      "|Hometown Foods / ...|     147|\n",
      "|    Hy-Vee / Corydon|      29|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### check to see if Zip Code value is null or blank\n",
    "spark.sql(\"\"\"\n",
    "    SELECT `Store Name`,\n",
    "    count(*)\n",
    "    FROM sales\n",
    "    where Address='' or Address is null\n",
    "    group by 1\n",
    "    order by `Store Name`\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Some store names have null for addess, zip, etc, but that's fine.  As long as we know that some won't join to the zip code demographics table, we're okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "1) products table - this is a fact table of all the liquiors sold in 2019.\n",
    "\n",
    "2) stores table - this is a fact table of all the stores that sold liquor in 2019.  This table will have population and medina income information based on the zipcode of the store for market research purposes.\n",
    "\n",
    "3) sales_summary - this is the monthly retail sales for 2019.  This is created so the data visualization team can feed this into Tableau\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1) First, I'm going to make the two dimension tables and the sales summary table as a Spark dataframe using SparkSQL\n",
    "\n",
    "2) Then I'm going to convert that into a pandas dataframe\n",
    "\n",
    "3) I will create three empty tables in Redshift\n",
    "\n",
    "4) Finally, I'm going to copy the dataframes from 2) into the empty tables in 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create Spark dataframes using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "products=spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    Category,\n",
    "    `Category Name`,\n",
    "    `Vendor Number`,\n",
    "    `Vendor Name`,\n",
    "    `Item Number`,\n",
    "    `Item Description`,\n",
    "    `Bottle Volume (ml)`\n",
    "    FROM sales\n",
    "    where Category is not null\n",
    "    group by \n",
    "    Category,\n",
    "    `Category Name`,\n",
    "    `Vendor Number`,\n",
    "    `Vendor Name`,\n",
    "    `Item Number`,\n",
    "    `Item Description`,\n",
    "    `Bottle Volume (ml)`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "stores=spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    `Store Number`,\n",
    "    `Store Name`,\n",
    "    Address,\n",
    "    City,\n",
    "    a.`Zip Code`,\n",
    "    Median,\n",
    "    Pop\n",
    "    FROM sales a \n",
    "    left join zip_median b\n",
    "    on a.`Zip Code` =b.Zip\n",
    "    --where Address is not null \n",
    "    group by\n",
    "    `Store Number`,\n",
    "    `Store Name`,\n",
    "    Address,\n",
    "    City,\n",
    "    a.`Zip Code`,\n",
    "    Median,\n",
    "    Pop\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sales_summary=spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    month(TO_DATE(CAST(UNIX_TIMESTAMP(Date, 'MM/dd/yyyy') AS TIMESTAMP))) as mnth,\n",
    "    sum(`Sale (Dollars)`)\n",
    "    FROM sales\n",
    "    group by \n",
    "    month(TO_DATE(CAST(UNIX_TIMESTAMP(Date, 'MM/dd/yyyy') AS TIMESTAMP)))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Then convert them into Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 ms, sys: 4.84 ms, total: 38.4 ms\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pandas_products = products.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# exported to csv to check data\n",
    "#pandas_products.to_csv('sample.csv', index=False, header=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 ms, sys: 1.52 ms, total: 23.5 ms\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pandas_stores = stores.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pandas_sales_summary = sales_summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Next several cells are to create a Redshift cluster using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwhCluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param       Value\n",
       "0        DWH_CLUSTER_TYPE  multi-node\n",
       "1           DWH_NUM_NODES           4\n",
       "2           DWH_NODE_TYPE   dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
       "4                  DWH_DB         dwh\n",
       "5             DWH_DB_USER     dwhuser\n",
       "6         DWH_DB_PASSWORD    Passw0rd\n",
       "7                DWH_PORT        5439\n",
       "8       DWH_IAM_ROLE_NAME     dwhRole"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::945626161665:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ClusterAlreadyExists) when calling the CreateCluster operation: Cluster already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-c5a636bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                   Value  \n",
       "0  dwhcluster                                                                             \n",
       "1  dc2.large                                                                              \n",
       "2  available                                                                              \n",
       "3  dwhuser                                                                                \n",
       "4  dwh                                                                                    \n",
       "5  {'Address': 'dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-c5a636bd                                                                           \n",
       "7  4                                                                                      "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::945626161665:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-080cb2e80b66538df')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "drop table products;\n",
    "CREATE TABLE IF NOT EXISTS products (\n",
    "Category int,\n",
    "Category_Name varchar,\n",
    "Vendor_Number int,\n",
    "Vendor_Name\tvarchar,\n",
    "Item_Number int,\n",
    "Item_Description varchar,\n",
    "Bottle_Volume_ml int\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "--drop table stores;\n",
    "CREATE TABLE IF NOT EXISTS stores (\n",
    "Store_Number int,\n",
    "Store_Name varchar,\n",
    "Address varchar,\n",
    "City varchar,\n",
    "Zip_Code int,\n",
    "Median_income int,\n",
    "Population int\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "--drop table sales_summary;\n",
    "CREATE TABLE IF NOT EXISTS sales_summary (\n",
    "month int,\n",
    "sales float\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Insert data into empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 ms, sys: 13.1 ms, total: 55 ms\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "con = sqlalchemy.create_engine(conn_string)\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "filename = 'dend-buket-fujimoto2/capstone/pandas_products.csv'\n",
    "with s3.open(filename, 'w') as f:\n",
    "    pandas_products.to_csv(f, index=False, header=False)\n",
    "\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    DELETE products;\n",
    "    COPY products\n",
    "    from 's3://%s'\n",
    "    iam_role 'arn:aws:iam::945626161665:role/dwhRole'\n",
    "    csv;\"\"\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 ms, sys: 2.92 ms, total: 36.4 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filename = 'dend-buket-fujimoto2/capstone/pandas_stores.csv'\n",
    "with s3.open(filename, 'w') as f:\n",
    "    pandas_stores.to_csv(f, index=False, header=False)\n",
    "\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    DELETE stores;\n",
    "    COPY stores\n",
    "    from 's3://%s'\n",
    "    iam_role 'arn:aws:iam::945626161665:role/dwhRole'\n",
    "    csv;\"\"\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 ms, sys: 4.48 ms, total: 27.4 ms\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filename = 'dend-buket-fujimoto2/capstone/pandas_sales.csv'\n",
    "with s3.open(filename, 'w') as f:\n",
    "    pandas_sales_summary.to_csv(f, index=False, header=False)\n",
    "\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    DELETE sales_summary;\n",
    "    COPY sales_summary\n",
    "    from 's3://%s'\n",
    "    iam_role 'arn:aws:iam::945626161665:role/dwhRole'\n",
    "    csv;\"\"\" % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking products table; below is a query to check rows per unique item_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>it_ct</th>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>3331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3</td>\n",
       "        <td>37</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 3331), (2, 573), (3, 37)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select it_ct, count(*) from (\n",
    "select \n",
    "item_number,\n",
    "count(*) as it_ct\n",
    "from products\n",
    "group by 1\n",
    ") a group by 1\n",
    "order by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select \n",
    "item_number,\n",
    "count(*) as it_ct\n",
    "from products\n",
    "group by 1\n",
    "having count(*)=2 and count(distinct bottle_volume_ml)=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>category</th>\n",
       "        <th>category_name</th>\n",
       "        <th>vendor_number</th>\n",
       "        <th>vendo_name</th>\n",
       "        <th>item_number</th>\n",
       "        <th>item_description</th>\n",
       "        <th>bottle_volume_ml</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1071100</td>\n",
       "        <td>Cocktails /RTD</td>\n",
       "        <td>395</td>\n",
       "        <td>PROXIMO</td>\n",
       "        <td>59159</td>\n",
       "        <td>1800 Ultimate Raspberry Margarita</td>\n",
       "        <td>1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1071100</td>\n",
       "        <td>Cocktails /RTD</td>\n",
       "        <td>395</td>\n",
       "        <td>PROXIMO</td>\n",
       "        <td>59159</td>\n",
       "        <td>1800 Ultimate Raspberry</td>\n",
       "        <td>1750</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1071100, 'Cocktails /RTD', 395, 'PROXIMO', 59159, '1800 Ultimate Raspberry Margarita', 1750),\n",
       " (1071100, 'Cocktails /RTD', 395, 'PROXIMO', 59159, '1800 Ultimate Raspberry', 1750)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select \n",
    "*\n",
    "from products \n",
    "where item_number=59159\n",
    "limit 10\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There seems be some item_numbers that have more than one row.  Some have different item_descrition and others have different bottle_volume_ml.  Since I am not an SME, I will keep all the records for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking stores table; below is a query to check rows per unique item_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "        <th>count_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1826</td>\n",
       "        <td>1748</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1826, 1748)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select \n",
    "count(*),\n",
    "count(distinct store_number)\n",
    "from stores \n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "4 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>it_ct</th>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3</td>\n",
       "        <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4</td>\n",
       "        <td>1</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 1686), (2, 47), (3, 14), (4, 1)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select it_ct, count(*) from (\n",
    "select \n",
    "store_number,\n",
    "count(*) as it_ct\n",
    "from stores\n",
    "group by 1\n",
    ") a group by 1\n",
    "order by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "47 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>store_number</th>\n",
       "        <th>it_ct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5711</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5774</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5709</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4779</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4859</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4947</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5106</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5245</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4533</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2585</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3542</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4084</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5803</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5618</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2666</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5473</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4406</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4969</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4067</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4091</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4777</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5705</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5760</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5565</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4647</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5801</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5719</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4767</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5277</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5700</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4725</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4424</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5564</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2596</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5205</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4584</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4480</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2848</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4654</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3930</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4351</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4609</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4468</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2693</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5487</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4320</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5698</td>\n",
       "        <td>2</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(5711, 2),\n",
       " (5774, 2),\n",
       " (5709, 2),\n",
       " (4779, 2),\n",
       " (4859, 2),\n",
       " (4947, 2),\n",
       " (5106, 2),\n",
       " (5245, 2),\n",
       " (4533, 2),\n",
       " (2585, 2),\n",
       " (3542, 2),\n",
       " (4084, 2),\n",
       " (5803, 2),\n",
       " (5618, 2),\n",
       " (2666, 2),\n",
       " (5473, 2),\n",
       " (4406, 2),\n",
       " (4969, 2),\n",
       " (4067, 2),\n",
       " (4091, 2),\n",
       " (4777, 2),\n",
       " (5705, 2),\n",
       " (5760, 2),\n",
       " (5565, 2),\n",
       " (4647, 2),\n",
       " (5801, 2),\n",
       " (5719, 2),\n",
       " (4767, 2),\n",
       " (5277, 2),\n",
       " (5700, 2),\n",
       " (4725, 2),\n",
       " (4424, 2),\n",
       " (5564, 2),\n",
       " (2596, 2),\n",
       " (5205, 2),\n",
       " (4584, 2),\n",
       " (4480, 2),\n",
       " (2848, 2),\n",
       " (4654, 2),\n",
       " (3930, 2),\n",
       " (4351, 2),\n",
       " (4609, 2),\n",
       " (4468, 2),\n",
       " (2693, 2),\n",
       " (5487, 2),\n",
       " (4320, 2),\n",
       " (5698, 2)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select \n",
    "store_number,\n",
    "count(*) as it_ct\n",
    "from stores\n",
    "group by 1\n",
    "having count(*)=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>store_number</th>\n",
       "        <th>store_name</th>\n",
       "        <th>address</th>\n",
       "        <th>city</th>\n",
       "        <th>zip_code</th>\n",
       "        <th>median_income</th>\n",
       "        <th>population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5774</td>\n",
       "        <td>Casey&#x27;s General Store #2185 / Manchester</td>\n",
       "        <td>908 W Main St</td>\n",
       "        <td>Manchester</td>\n",
       "        <td>52057</td>\n",
       "        <td>46013</td>\n",
       "        <td>8222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5774</td>\n",
       "        <td>Casey&#x27;s General Store #2185 / Manchester</td>\n",
       "        <td>1305 W Commercial St</td>\n",
       "        <td>Manchester</td>\n",
       "        <td>52057</td>\n",
       "        <td>46013</td>\n",
       "        <td>8222</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(5774, \"Casey's General Store #2185 / Manchester\", '908 W Main St', 'Manchester', 52057, 46013, 8222),\n",
       " (5774, \"Casey's General Store #2185 / Manchester\", '1305 W Commercial St', 'Manchester', 52057, 46013, 8222)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select \n",
    "*\n",
    "from stores \n",
    "where store_number=5774\n",
    "limit 10\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, there seems be some store_numbers that have more than one row.  Some have different addresses and others have different store_name.  Since I am not an SME, I will keep all the records for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "12 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>month</th>\n",
       "        <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>23466782.5899997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>24641910.4899993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3</td>\n",
       "        <td>24982728.3599986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>4</td>\n",
       "        <td>27788607.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>5</td>\n",
       "        <td>32756773.2699958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>6</td>\n",
       "        <td>28877030.9599999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>7</td>\n",
       "        <td>30967399.6899979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>8</td>\n",
       "        <td>28424263.2899975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>9</td>\n",
       "        <td>28078111.230002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>10</td>\n",
       "        <td>34448213.3499975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11</td>\n",
       "        <td>30270164.5099978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>12</td>\n",
       "        <td>34518356.0399999</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 23466782.5899997),\n",
       " (2, 24641910.4899993),\n",
       " (3, 24982728.3599986),\n",
       " (4, 27788607.85),\n",
       " (5, 32756773.2699958),\n",
       " (6, 28877030.9599999),\n",
       " (7, 30967399.6899979),\n",
       " (8, 28424263.2899975),\n",
       " (9, 28078111.230002),\n",
       " (10, 34448213.3499975),\n",
       " (11, 30270164.5099978),\n",
       " (12, 34518356.0399999)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * From sales_summary order by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Great!  All 12 months are generated and the sales numbers look good.\n",
    "\n",
    "An insight would be the sales volume is high in the summer, as well as Q4 when there are a lot of holidays and events where people tend to drink a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Products table**\n",
    "\n",
    "category - category code\n",
    "\n",
    "category_name - category name of product\n",
    "\n",
    "vendor_number - vendor id\n",
    "\n",
    "vendor_name - name of company that produces the product\n",
    "\n",
    "item_number - item id\n",
    "\n",
    "item_description - \tname of product\n",
    "\n",
    "bottle_volume_ml - volume in ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Stores table**\n",
    "\n",
    "store_number - unique store id\n",
    "\n",
    "store_name - name of store\n",
    "\n",
    "address - address\n",
    "\n",
    "city - city\n",
    "\n",
    "zip_code - zip code of store\n",
    "\n",
    "median_income - median income of zip code\n",
    "\n",
    "population - population of zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Sales_Summary table**\n",
    "\n",
    "month - month identifier of 2019\n",
    "\n",
    "sales - sales by month\n",
    "\n",
    "This table is to be fed into a BI tool like Tableau so executives and decicison makers can get high level metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Tools**\n",
    "\n",
    "Spark and Redshift were selected as a tool since the sales records csv was easily over 2M records.  Spark made it very fast to read the records and crunch the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Update Frequency**\n",
    "\n",
    "The sales data is updated by the State of Iowa every month.  Data should be updated after the source is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Scenarios**\n",
    "\n",
    "If the data increased by 100x, I would probably use Spark in AWS EMR and tunnel it to a linux box.\n",
    "\n",
    "If the data needed to be updated every day at 7AM (assuming the source would be updated everyday), I would use Airflow to schedule the summary and the feed to Redshift.\n",
    "\n",
    "If the data needed to be accessed by 100+ people, I would still put it in Redhshift since more people are familar with SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Delete Clusters and connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'dwhuser',\n",
       "  'DBName': 'dwh',\n",
       "  'Endpoint': {'Address': 'dwhcluster.chuxjgcjz9kt.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2020, 1, 31, 19, 2, 51, 632000, tzinfo=tzlocal()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-2dfbb66b',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-c5a636bd',\n",
       "  'AvailabilityZone': 'us-west-2c',\n",
       "  'PreferredMaintenanceWindow': 'sun:08:00-sun:08:30',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::945626161665:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2020, 2, 2, 8, 0, tzinfo=tzlocal())},\n",
       " 'ResponseMetadata': {'RequestId': 'bdcdc3b6-4481-11ea-9263-55ac880d36d4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bdcdc3b6-4481-11ea-9263-55ac880d36d4',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2382',\n",
       "   'vary': 'Accept-Encoding',\n",
       "   'date': 'Fri, 31 Jan 2020 23:31:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>deleting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-c5a636bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key           Value\n",
       "0  ClusterIdentifier  dwhcluster    \n",
       "1  NodeType           dc2.large     \n",
       "2  ClusterStatus      deleting      \n",
       "3  MasterUsername     dwhuser       \n",
       "4  DBName             dwh           \n",
       "5  Endpoint           {'Port': 5439}\n",
       "6  VpcId              vpc-c5a636bd  \n",
       "7  NumberOfNodes      4             "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
